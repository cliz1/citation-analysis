

GoFetch: Breaking Constant-Time Cryptographic Implementations Using Data Memory-Dependent Prefetchers.
3. Breaking Constant-Time Cryptography. Undergirded by our chosen-input attack framework, in Sections 6 and 7 we develop end-to-end key-extraction attacks on constant- time implementations of classical cryptography (OpenSSL Diffie-Hellman Key Exchange and Go RSA decryption) and post-quantum cryptography (CRYSTALS-Kyber and CRYSTALS-Dilithium). 1.2 Disclosure We disclosed to Apple, OpenSSL, Go Crypto, and the CRYS- TALS team. Apple is investigating our PoC. OpenSSL re- ported that local side-channel attacks (i.e., ones where an attacker process runs on the same machine) fall outside of their threat model. The Go Crypto team considers this attack to be low severity. The CRYSTALS team agreed that pinning to the Icestorm cores without DMP could be the short-term solution and hardware fixes are needed in the long term. 2 Background Cache Architecture. Modern processors use a hierarchy of caches to reduce memory access latency. Typically, higher- level caches are smaller and faster to access, while lower-level caches are larger but slower to access. For example, the Apple processors we study in this paper have two cache levels, a core- private L1 and a shared L2. These caches are set-associative, meaning that they contain a fixed number of cache sets, each of which can fit a fixed number of cache lines. Cache lines are the basic unit for cache transactions. Multi-level caches have an inclusion policy that determines how the presence of a cache line in one level affects its presence in other levels. Most of our experiments were conducted on the Apple M1’s 4 Firestorm (performance) cores, which are the only ones to have a DMP. Each Firestorm core has a 128 KByte, 8 way set- associative L1 data cache with 64 Byte cache lines and these 4 Firestorm cores share a 12 MByte, 12 way set-associative L2 data cache with 128 Byte cache lines. The shared L2 cache is inclusive of the L1 caches, i.e. every cache line present in the L1 is also present in the L2 [94]. Cache Side-Channel Attacks. In a cache side-channel at- tack, an attacker infers a victim program’s secret by observing the side effects of the victim program’s secret-dependent ac- cesses to the processor cache. These attacks typically consist of three steps, during which the attacker (i) brings the cache into a known state, (ii) lets the victim execute, and (iii) checks the state of the cache to learn information about the victim’s execution during step (ii). Two techniques commonly used to mount cache side-channel attacks are Flush+Reload


[92] and Prime+Probe [64]. In Flush+Reload, an attacker that shares memory with a victim flushes individual shared cache lines and later reloads them to figure out if the victim accessed them. In Prime+Probe, the attacker builds an eviction set of addresses that map to the same cache set as the victim’s target cache line, primes the cache set with the eviction set, and later probes it to figure out whether the victim accessed the target line / displaced a line in the eviction set. Classical Prefetchers. Prefetchers are a hardware opti- mization used to hide memory access latency. Prefetchers live in the memory system, typically between the L1 and L2 or between the L2 and DRAM, and work by pre-loading data into the cache before it is requested by the core. In particular, given a program memory access pattern, classical prefetchers try to predict the next addresses the program will access based on its access pattern (an address trace) thus far. Classical Prefetcher Security Implications. Several prior works have analyzed the security implications of classical 1118    33rd USENIX Security Symposium USENIX Association prefetchers [17, 26, 27, 31, 76, 91, 98]. These works demon- strate that, through unintended interactions with prefetchers, victim programs can create cache state changes that can be measured by the attacker to leak information. Fortunately, leakage through these attacks is limited to the victim’s access pattern and can be mitigated through constant-time program- ming practices that ensure the program memory access pattern does not depend on secrets. Data Memory-Dependent Prefetchers (DMPs). DMPs are a class of prefetchers designed to prefetch irregular memory access patterns. In contrast to classical prefetchers, which only take the memory access pattern as an input, DMPs also take into account the contents of data memory directly to determine what to prefetch. The computer architecture literature and industry patents proposed several types of DMPs [7,8,16,24, 29,50,84,96,97], which differ in the irregular access patterns that they are designed to speed up (e.g., linked-list traversals, sparse matrix traversals). DMP Security Implications. Vicarte et al. were the first to perform an analysis of the security implications of DMPs [72]. In the worst case, they found that proposed (but not known to be implemented) indirect memory prefetchers could be used to build universal read gadgets that leak a program’s entire memory, similar to Spectre [52, 60]. More recently, Augury demonstrated that modern Apple processors employ a type of DMP referred to as an Array-of-Pointers (AoP) DMP [84]. We describe this DMP’s behavior in more detail in Section 4.1. 3 Threat Model and Setup In this paper we assume a typical microarchitectural attack scenario, where the victim and attacker have two different processes co-located on the same machine. Software. For our cryptographic attacks, we assume the attacker runs unprivileged code and is able to interact with the victim via nominal software interfaces, triggering it to perform private key operations. Next, we assume that the victim is constant-time software that does not exhibit any (known) microarchitectural side-channel leakage. Finally, we assume that the attacker and the victim do not share memory, but that the attacker can monitor any microarchitectural side channels available to it, e.g., cache latency. As we test unpriv- ileged code, we only consider memory addresses commonly allocated to userspace (EL0) programs by macOS. Hardware. Unless otherwise specified, we focus on Apple hardware. The M1-based experiments of Section 4 are run on a Mac Mini with an Apple M1 running macOS 13.5. For our investigation into the M2/M3 microarchitecture, we used a Mac Mini with an Apple M2 (running macOS 14.2.1) and a MacBook Pro with an Apple M3 (running macOS 14.2). Finally, when investigating Intel’s DMP implementation, we used an Intel Core i9-13900K (Raptor Lake) CPU, running Ubuntu 23.04 with kernel version 6.2.0. Prefetched by stream prefetcher Dereferenced by code Dereferenced by DMP dummy dummy dummy … This Work ptr[0] ptr[1] ptr[2] ptr[M-1] … This Work * * * * ptr[0] ptr[1] ptr[2] ptr[N-1] … Augury ptr[N] * * * * * ptr[M-1] * … ptr[M-1] … * dummy ptr[N] * ptr[0] * This Work ptr[1] ptr[7] … Loaded alongside (within same cache line) * * Figure 1: We compare memory access patterns and subse- quent prefetches. The first row represents the activation pat- tern reported by Augury [84]: a streaming dereference access pattern causes the DMP to dereference out-of-bounds pointers. In the second row, we show that architectural/program-level dereferences are unnecessary; we see DMP activations even when the training array contains non-pointer values. In the third row, we show that the DMP even dereferences the in- bounds pointers that are architecturally accessed (but, again, not dereferenced). Finally, the last row shows that a single access to a memory location results in all pointers stored in the incident cache line being dereferenced. 4 Microarchitectural Characterization 4.1 Revisiting DMP Data Access Patterns In this section, we investigate the access patterns required to activate the M1 DMP. We show that the M1 DMP deref- erences more pointers and with fewer program assumptions than was claimed by Augury [84]. Figure 1 summarizes the subsection’s findings. Augury. We begin by reviewing the M1 DMP activation pattern and methodology described in Augury. Augury’s code, summarized in Listing 1 (left), first allocates an array (aop) of length M and fills aop with pointers to memory addresses that correspond to unique L2 cache lines. Next, it evicts these cache lines from the L2 via cache thrashing (by loading an ar- ray eight times the size of the cache). The code then accesses (loads) and dereferences the first N elements of the aop, where N ≤M. We call aop[0], ..., aop[N-1] the in-bounds point- ers and aop[N], ..., aop[M-1] the out-of-bounds pointers. Augury inferred the DMP’s activity by adding code after the loop to time how long it would take to dereference pointers in the aop. We call these test accesses. The main finding was that the latency of test accesses for out-of-bounds pointers in some index range [N,N +δ) corresponded to L2 cache hits. This is noteworthy because the code itself never dereferenced pointers located after aop[N]. Augury attributed this behavior USENIX Association 33rd USENIX Security Symposium    1119 to a new form of prefetcher, with prefetch distance δ. uint64_t* aop[M]; // Fill aop with pointers // to unique addresses // or random values for (i=0; i<N; i++) { *aop[i%N]; } // Measure latency to // set of test addresses uint64_t* aop[M]; // Fill aop with pointers // to unique addresses // or random values for (i=0; i<N; i++) { aop[i%N]; } // Measure latency to // set of test addresses Listing 1: Left: The DMP activation code pattern studied by Augury [84]. Right: The DMP activation pattern studied in this work. For both, assume N ≤M. Both code patterns fill the aop before the loop begins and use a mod operation to inhibit speculative execution. Observing DMP Activations. We reproduce Augury’s ex- periments by setting N = 256 and M = 264, choosing a set of test pointers, and then either filling the out-of-bounds region with those pointers or random values. When the pointers are present, a test access (dereference) to one takes ∼250 cycles,1 as shown in Figure 2a. When the pointers are not present, the same test accesses take significantly longer. A cutoff of 300 cycles (red dash line) cleanly differentiates between the two cases and thus DMP activations. This corresponds to the L2 hit time and matches Augury’s findings, consistent with δ ≥8. Avoiding Architectural Pointer Dereferencing. To deter- mine if the architectural pointer dereferences are required to trigger DMP activations we use the code in Listing 1 (right), where the in-bounds region does not contain pointers nor does the aop traversal loop perform any pointer dereferences. Again, we either fill the out-of-bounds region with test point- ers or random values. See Figure 1 (second row). As seen in Figure 2b, when the out-of-bounds region con- tains pointers, test accesses are < 300 cycles despite no ar- chitectural dereferences occurring to the in-bounds pointers. From this, we deduce that architectural dereferences are not re- quired for the DMP to activate, i.e., that the DMP will prefetch out-of-bounds pointers without them. In-bounds DMP Dereferencing. We then further check if the in-bounds pointers are also dereferenced by the DMP as they are no longer architecturally dereferenced in Listing 1 (right). This is the memory access pattern outlined in Figure 1 (third row), where we iterate over an array containing valid pointers without performing any dereferences. Figure 2c shows that for N = 8, we can still consistently differentiate between the two cases. This indicates that if the aop contains data which can be interpreted as valid pointers, merely iterating over it is sufficient to activate the DMP. One Load, Single Pointer. Finally, we consider how general the memory access pattern can be by performing a single data 1We collect timing measurements by configuring and reading performance counters (PMC2-PMC7) for cycle counting via kperf. 256 257 258 259 260 261 262 263 Test Index 0 100 200 300 400 500 600 700 800 Access Latency (cycles) No Pointer Contain Pointer (a) Row 1: Traversing the AoP with dereferences; out-of-bounds pointers are prefetched 256 257 258 259 260 261 262 263 Test Index 0 100 200 300 400 500 600 700 800 Access Latency (cycles) No Pointer Contain Pointer (b) Row 2: Traversing the AoP without dereferences; out-of- bounds pointers are prefetched 0 1 2 3 4 5 6 7 Test Index 0 100 200 300 400 500 600 700 800 Access Latency (cycles) No Pointer Contain Pointer (c) Row 3: Traversing the AoP without dereferences; in-bounds pointers are prefetched 0 1 2 3 4 5 6 7 Test Index 0 100 200 300 400 500 600 700 800 Access Latency (cycles) No Pointer Contain Pointer (d) Row 4: One load to AoP; pointers within the incident cache line are prefetched Figure 2: Median, minimum, and maximum test access laten- cies (over 32 samples for each bar) using the access patterns of Figure


8. In all cases, we observe DMP activations/dereferences for all pointers in aop, indicating that even a single pointer can trigger the DMP. 4.2 DMP Activation Criteria Having established what memory access patterns activate the DMP, this section investigates where data must reside in the memory hierarchy to be DMP-searched for pointers. We show that the DMP dereferences pointers specifically on L1 cache fills and features two mechanisms to prevent redundant prefetches: a history filter and a do-not-scan hint. In this section, we make use of standard eviction sets, i.e., eviction sets for individual cache sets. We generate these eviction sets using standard techniques from prior work [85].3 History Filter. We start by rerunning the experiments from 2Replacing the load with the store instruction, we find that none of point- ers in the accessed cache line are dereferenced. 3This is in contrast with Augury, which, as we mentioned in Section 4.1, relied on cache thrashing to precondition the cache. 1120    33rd USENIX Security Symposium USENIX Association Section 4.1 using standard L2 eviction sets to evict both the aop array and the L2 cache lines that are pointed to by pointers in the aop array. We call these L2 lines the target lines. We observe that the DMP only reliably dereferences each pointer once, on the first access to its aop entry. That is, even if the previously prefetched target line is evicted from the cache, along with its aop entry, the DMP no longer activates when seeing that pointer in the future. This observation suggests that the decision to dereference a pointer is made based on not only the program’s access pattern but also some additional mechanism. An Apple patent suggests that this mechanism might be a history filter that “attempts to identify whether a given memory pointer candidate likely corresponds to a candidate that has been recently prefetched, in which case the given candidate may be discarded as a likely duplicate” [47]. The same patent suggests that this filter may be organized as a direct-mapped 128-entry or 256-entry structure. History Filter Reverse Engineering. To corroborate the history filter hypothesis, we design a new experiment where aop only contains a single pointer ptr. First, we access aop, causing the DMP to dereference ptr. We then evict aop and the target line for ptr from the cache using standard eviction sets. Next, we read S unique pointers stored in a different array, causing the DMP to inspect and dereference S additional pointers. Finally, we re-access aop and check if this second access causes the DMP to dereference ptr. We run the experiment 100 times for each value of S and report the success rate (i.e., the percentage of times that the DMP activated on the second aop access) in Figure 3. 1 2 4 8 16 32 64 128 256 Number of different ptrs 0 10 20 30 40 50 60 70 80 Success Rate (%) Figure 3: The percentage of experiments where the DMP re-activates when ptr is re-accessed (Success Rate, y-axis), as a function of the number of unique pointers accessed in between the first and second access to ptr (x-axis). Observe that Success Rate increases with the number of unique inter- mediate pointer accesses. We observe that the DMP only reliably re-activates on ptr when S ≥128. This behavior is likely due to the lim- ited capacity of the history filter. That is, accessing S unique pointers results in the record of ptr’s target getting evicted from the filter when S ≥128. We hypothesize that Augury’s methodology was not affected by the history filter because its aggressive cache thrashing technique (i.e., accessing an array eight times the size of the cache) had a side effect of also flushing the history filter. We further find that the history filter is a per-core structure and is reset if a core remains idle for an extended period of time. Specifically, the DMP reliably re-activates even when S = 0 if we (i) reschedule our experiment to a different core between the first and the second aop access or (ii) run the experiment on one core but leave the core idle for 100µs or more between the first and the second aop access. L1 and L2 Cache Fills. The above observations indicate that the DMP activates when an aop entry is accessed from DRAM and the record of its target is not present in the history filter. Next, we investigate at which stage of a DRAM fetch the DMP scans the data for pointers. Recall that the M1 has an L2 line size of 128 Bytes and an L1 line size of 64 Bytes. With each pointer containing 8 Bytes, L2 lines can thus be split into “lower” and “upper” halves, each of which is an independent L1 line that can store 64/8 = 8 pointers. When a program accesses either the lower or upper half, the accessed L1 line will be filled into both the L1 and L2 caches, while the other half will only be filled into the L2 cache.4 In order to differentiate between L1 and L2 fills, we populate a L2 line size-aligned aop with 16 unique pointers and run the experiment from Listing 1 (right) in Section 4.1 with N = 1 and M =


16. Before each repetition, we use cache thrashing (as in Section 4.1) to evict the aop and its target lines from both the cache and the history filter. Figure 4 (top) summarizes our findings, repeating each ex- periment 100 times and using the 300 cycle threshold from Section 4.1 for L2 cache hits. Here, we observe that when the program accesses aop[0], the DMP only dereferences aop[0], ···, aop[7]. We run 7 more variants of this experi- ment, varying the single aop[i] access from i = 1,··· ,7 and observe the same behavior for each choice of i. Next, we run 8 more variants of the same experiment, this time making a single access to aop[i] for i = 8,··· ,15. In this case, we ob- serve that aop[8], ···, aop[15] are all dereferenced for each choice of i, as shown in Figure 4 (bottom). We conclude that when filling an L2 cache line from DRAM, the DMP derefer- ences all pointers in the specific L1 line that is accessed, and not those in the other half of the L2 line. We run 8 more variants of the above experiment. For these, before making an access to aop[i] for i = 0,··· ,7, we first make an access to aop[8]. We then repeat this setup while ex- ploring the opposite case: before making an access to aop[i] for i = 8,··· ,15, we first make an access to aop[0]. As dis- cussed above, the first access brings aop[i] from DRAM to the L2 cache and aop[i] further moves to the L1 cache with the second access. We observe that the DMP reliably dereferences the contents of the L1 line containing aop[i]. This means that L2 to L1 fills can also activate the DMP. Do-not-scan Hint. The above experiments suggest that 4We empirically verify this by subsequently timing an access to the other half and observing that its access latency corresponds to the that of an L2 hit. USENIX Association 33rd USENIX Security Symposium    1121 0 50 100 Success Rate (%) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Position Offset 0 50 100 Success Rate (%) Figure 4: Which pointers in an L2 line are dereferenced when an access is made to data in that line? Top: the code accesses aop[0]. Bottom: the code accesses aop[8]. We conclude that the DMP dereferences pointers in the specific L1 line (either the upper or lower half of the L2 line) the code accessed. the DMP searches for pointers in L1 cache lines during L1 fills, regardless of whether the L1 line is fetched from DRAM or the L2. To corroborate this hypothesis, we design another variant of the single-pointer experiment from Section 4.1. The experiment starts by loading the aop into the L1 and subsequently using eviction sets to either (i) evict the aop from the L1 or (ii) evict the aop from both the L1 and L2. In both cases, the experiment also evicts the target line from the cache and accesses a separate set of 256 pointers to evict the record of the target line from the history filter. Finally, the experiment re-accesses aop and tests if this second aop access causes the DMP to re-dereference ptr. Interestingly, we observe that the DMP does not re- dereference ptr when the experiment re-accesses aop and aop was only evicted from the L1. However, when the aop is also evicted from the L2, the DMP re-dereferences it. This means that even if the previously prefetched target line is evicted from both the cache and the history filter, the DMP does not dereference that pointer again unless its aop entry is also evicted from both the L1 and L2. This behavior matches a mechanism also described in the previously referenced Ap- ple patent [47], where the L2 sets a “do-not-scan” hint on L1 cache fills to prevent a previously scanned L1 cache line from being redundantly re-scanned. Fortunately, in our ex- periments, evicting the aop from both the L1 and the L2 is sufficient to clear the “do-not-scan” hint on the aop. 4.3 Restrictions on Dereferenced Pointers In the previous section, we learned that the DMP activates on L1 fills and dereferences the pointers inside it if and only if those pointers’ targets are not in the history filter and the filled line is not marked with the "do-not-scan" hint. We now investigate what pointers can be dereferenced by the DMP. For this, we again use Listing 1 (right) with N = 1 and M = 1 and rely on cache thrashing to ensure that the aop is uncached. We then try testing different pointer values in the aop, and checking for DMP activations. 4GByte Prefetch Region. We begin by investigating if the DMP requires there to be a relationship between the ad- dress of the aop entry and the value of the aop entry (i.e., the pointer). We call the address of the aop entry the en- try’s/pointer’s position. To understand what the requirements are for one pointer to be dereferenced, we carry out a series of experiments that vary a pointer’s position and value. See one such experiment in Figure 5 which shows that the pointer’s position and value must be related for DMP activation to oc- cur. Overall, we discover that the DMP only dereferences a pointer if the aop entry and target line are in the same 4 GByte- aligned region (Figure 6). In other words, that the upper 32 bits of their addresses match. Apple’s patent


63. We observe that the DMP does not 1122    33rd USENIX Security Symposium USENIX Association * target aop … 4GByte Bound aop * Figure 6: Outline of the placement of the target line and the aop entry. Following the observation from Figure 5, if the aop entry and target line straddle a 4GByte boundary, the DMP won’t dereference the pointer. dereference the original pointer if a bit in the range [48,55] is flipped. However, if a bit in the range [56,63] is flipped, the original pointer gets dereferenced. We conclude that the DMP ignores the upper 8 bits of a pointer when dereferencing it, which matches the “Top-Byte-Ignore” in ARMv8. 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 Flip Bit Index 0 20 40 60 80 100 Success Rate (%) Figure 7: Activation success rate for a pointer when it is accessed by the program, after having one bit flipped between bit 48 to 63. Auxiliary next-line prefetch. Finally, we investigate the amount of data prefetched when the DMP dereferences a pointer. We test this by performing a test access to not only a pointer’s target line, but also to nearby lines. Apart from the target line, we also observe L2 hits to cache lines immediately next to the target line. We hypothesize that this is due to a next-line prefetcher being triggered alongside the DMP, which matches the adjacent-line prefetch behavior described in Apple’s patent [47]. 4.4 A Model for the DMP’s Behavior We now summarize the previous two subsections and make several new observations. Step 1: Observing Cache Line Data. The DMP scans the data in an L1 line when that line is filled to the L1, if the line is not marked with the “do-not-scan” hint (i.e., the line has not been scanned since it was brought into the cache; Section 4.2). The DMP performs the scan by checking each pointer size-aligned chunk (the first 64 bits, second 64 bits, etc.) in the cache line.5 Step 2: Address Check. Next, the DMP applies additional checks and filters to each chunk (candidate pointer) to see if it should be dereferenced. Bits [63:56] are ignored (Section 4.3). 5Pointers in aop should be 64-bit aligned, which is also discussed in [84]. Further, per Section 4.3, the cache line that stores the pointer (its position) must be in the same 4 GByte (log2 4 GByte = 32 bits)-aligned region as the cache line that the pointer points to (its target). In other words, the DMP checks whether bits [55:32] of the candidate pointer match the corresponding bits of the address of the target cache line. Finally, the DMP checks if the candidate pointer is present in the history filter (Section 4.2). If bits [55:32] match and the pointer is not in the history filter, the DMP attempts to prefetch two L2 lines. Specifically, it first prefetches the cache line targeted by the 64-bit chunk, ignoring the top byte value. Next, it triggers the CPU’s next line prefetcher and fetches the neighboring cache line also into the CPU’s L2 cache (Section 4.3). Both prefetched addresses are then inserted to the history filter. As part of the prefetching process, the DMP looks up the translation lookaside buffer (TLB) and triggers page table walks to obtain the physical address corresponding to each candidate pointer (which is a virtual address [33]). On a TLB miss, the DMP inserts the missing translations into the TLB.6 4.5 Other Microarchitectures We investigated the DMP behavior on other microarchitec- tures including the Apple M2/M3 and Intel’s 13th Generation (Raptor Lake) CPUs, and display results in Figures 8a and 8b. As the Apple M3 behaves similarly to the M2, we omit its figure. In these two figures, the x-axis refers to the four access patterns shown as the rows in Figure 1, while the y-axis is the access latency for test accesses. For simplicity, we only show latencies for test accesses to the first pointer in each pattern. The Intel i9-13900K (Raptor Lake) shows a distin- guishable timing difference only for the first access pattern from Figure 1, whereas the M2/M3 activates on all the pat- terns discussed previously. We conclude that while DMPs are present on Raptor Lake machines, they require different acti- vation patterns. Finally, we leave the systematic investigation and exploration of Intel’s DMPs to future work. Row 1 Row 2 Row 3 Row 4 0 100 200 300 400 500 600 700 800 Access Latency (cycles) No Pointer Contain Pointer (a) Apple M2 Row 1 Row 2 Row 3 Row 4 0 100 200 300 400 500 600 700 800 Access Latency (cycles) No Pointer Contain Pointer (b) Intel Raptor Lake Figure 8: We test four access patterns shown in Figure 1 on Apple M2 (left) and Intel 13th generation Raptor Lake (right). 6Prior work


[84] also observes that the M1 DMP fills TLB entries for pointers in the aop. USENIX Association 33rd USENIX Security Symposium    1123 5 Attacking Constant-Time Conditional Swap To mitigate microarchitectural side channels, cryptographic code follows the constant-time programming principle: A secret should not determine which instructions to execute, which memory to access, or be used as input for variable-time instructions [11,13,14,21–23,62]. We now show how the DMP can break cryptographic secu- rity even when code is written to follow the constant-time prin- ciple. To introduce ideas and attacker tools, this section show- cases a Proof-of-Concept (PoC) attack on a core constant-time cryptographic primitive


[54] called ct-swap which condi- tionally swaps the contents of two arrays a and b based on a secret bit secret. We start with ct-swap to simplify the presentation. Later sections will reuse the ideas and processes described here to break real cryptographic code. Constant Time Swap Overview. Listing 2 swaps the contents of array a and b based on the value of secret in a constant-time manner. The underlying swap opera- tion for each 64-bit entry is borrowed from OpenSSL.7 To achieve constant-time behavior, Line 4 in Listing 2 first extends secret to be a machine-sized word; i.e., 0x0000000000000000 or 0xFFFFFFFFFFFFFFFF based on the value of secret. Next, for each loop iteration, Line 6 of Listing 2 computes a masked delta between the contents of the current elements of a and b. Finally, Lines 7 and 8 actually conditionally swap the contents of the two elements, based on the value of secret. 1 void ct-swap(uint64_t secret, uint64_t *a, uint64_t *b, 2 size_t len) { 3 uint64_t delta; 4 uint64_t mask = ~(secret-1); 5 for (size_t i = 0; i < len; i++) { 6 delta = (a[i] ^ b[i]) & mask; 7 a[i] = a[i] ^ delta; 8 b[i] = b[i] ^ delta; 9 } 10 } Listing 2: Code snippet of constant-time swap. The contents of a and b is conditionally swapped based on secret. 5.1 Attack Overview and Challenges Emulating realistic attack scenarios, we assume that ct-swap runs in a victim process, separate from the attacker’s address space. We assume a simple but common protocol between victim and attacker, where the victim takes input from the attacker to populate the ct-swap’s a and b arrays and then executes ct-swap. The outcome of the swap is never directly revealed, nor is the value of secret. The attacker can learn page offsets (not randomized by ASLR) of array a and b by 7constant_time_cond_swap_64: https://github.com/openssl/ openssl/blob/1751185154ab1f1a796e0f39567fe51c8e24b78d/ include/internal/constant_time.h. investigating the victim’s program in advance. The attacker process’ goal is to extract the value of secret from the victim, using microarchitectural side channels and the DMP. Chosen-Input Attack. We now overview how the attacker uses the DMP to extract secret. At a high level, the attacker populates one of ct-swap’s arrays (a or b—let us assume it chooses b) with a pointer ptr of its choosing, and then arranges for the DMP to dereference the contents of the other array (a) during the conditional swap computation. Then, the attacker uses conventional cache side-channel analysis to observe whether ptr was dereferenced by the DMP due to ct-swap’s computation over a, which in turn reveals whether the swap occurred and therefore the value of secret. Overcoming DMP Activation Criteria. To correctly at- tribute the DMP’s activation to ptr being moved from b to a, the attacker must ensure that the DMP’s activation criteria are only satisfied when accessing a (and not b). Based on Section 4.2, one necessary prerequisite to activate the DMP on an aop load is to evict the aop from the L2 cache. Thus, we need a means to evict a8 (but not b). Overcoming Address Space Separation. Yet, since the attacker runs in a separate process from the victim and without any shared memory, we must replace the Flush+Reload in Section 4 with Prime+Probe. In particular, we must build an eviction set to detect whether ptr was dereferenced by the DMP inside the victim process. However, it is not clear how to build eviction sets for ptr’s target line (or a mentioned above),9 as we cannot time accesses to these since they are located inside the victim’s address space. 5.2 Compound Eviction Set Construction We now present a novel technique—compound eviction set generation—which solves the above problem by using ct-swap’s access to a as well as DMP dereferences to ptr to simultaneously build eviction sets for both elements. Establishing a Timing Source. To start, we need to distin- guish between L2 hits and misses. However, as the attacker is running without elevated privileges, it is unable to access nanosecond-accurate timers on Apple CPUs, instead being limited to the system’s 42 ns timer. Unfortunately, we empir- ically find that this timer is not sufficient to reliably mount Prime+Probe attacks. We sidestep this issue by using the multi-thread timer approach of [46, 69, 73]. Here, the main idea is to use a dedicated counting thread, which constantly in- crements a shared variable with the attacker process in a tight loop. By loading the value of the shared variable, the attacker 8Triggering the DMP also requires that a is refilled after it is evicted. We rely on the victim to perform this refill. For example, ct-swap reads a in a loop, which will cause each cache line making up a to be accessed (refilled) multiple times (len > 1). 9We assume that the base addresses of a and b have different page-offset bits, so that the eviction set for a would not evict b, which also holds for later attacks. 1124    33rd USENIX Security Symposium USENIX Association process is thus able to obtain high resolution timestamps, allowing us to distinguish L2 hits from misses. Generating Standard Eviction Sets. Next, we need to gen- erate a large number of standard L2 eviction sets, i.e., eviction sets targeted to individual L2 sets. The M1 has 8192 (213) L2 cache sets, indexed with 6 (upper) bits from the physical page frame and 7 (lower) bits from the page offset. We generate standard eviction sets for all these L2 sets by extending the technique used in Section 4.2 (detailed in


